{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3a7575",
   "metadata": {},
   "source": [
    "# Training an SNN using surrogate gradients!\n",
    "\n",
    "This notebook contains the NMNIST results for arctan surrogate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7fa5fb-4958-48fa-adc5-d5f7cbe3202f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:28:55.899256Z",
     "iopub.status.busy": "2023-08-22T13:28:55.898988Z",
     "iopub.status.idle": "2023-08-22T13:28:57.183882Z",
     "shell.execute_reply": "2023-08-22T13:28:57.183376Z",
     "shell.execute_reply.started": "2023-08-22T13:28:55.899235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([148.08, 419.56], dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "warmup = jnp.array([12,34])\n",
    "warmup * 12.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf6fa71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:28:58.238872Z",
     "iopub.status.busy": "2023-08-22T13:28:58.238433Z",
     "iopub.status.idle": "2023-08-22T13:29:00.474416Z",
     "shell.execute_reply": "2023-08-22T13:29:00.473695Z",
     "shell.execute_reply.started": "2023-08-22T13:28:58.238846Z"
    }
   },
   "outputs": [],
   "source": [
    "# implement our SNN in DeepMind's Haiku\n",
    "import haiku as hk\n",
    "\n",
    "# JAX imports\n",
    "import jax\n",
    "import jmp\n",
    "\n",
    "# rendering tools\n",
    "# for surrogate loss training.\n",
    "import optax\n",
    "from jax import numpy as jnp\n",
    "from jax_tqdm import scan_tqdm\n",
    "\n",
    "import spyx\n",
    "import spyx.nn as snn\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9402d",
   "metadata": {},
   "source": [
    "## Set Mixed Precision Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfdcd58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:29:01.977227Z",
     "iopub.status.busy": "2023-08-22T13:29:01.976351Z",
     "iopub.status.idle": "2023-08-22T13:29:01.981005Z",
     "shell.execute_reply": "2023-08-22T13:29:01.980459Z",
     "shell.execute_reply.started": "2023-08-22T13:29:01.977198Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = jmp.get_policy('half')\n",
    "\n",
    "\n",
    "hk.mixed_precision.set_policy(hk.Linear, policy)\n",
    "hk.mixed_precision.set_policy(snn.LIF, policy)\n",
    "hk.mixed_precision.set_policy(snn.LI, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf2a89",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6573ba59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:29:03.580988Z",
     "iopub.status.busy": "2023-08-22T13:29:03.580705Z",
     "iopub.status.idle": "2023-08-22T13:30:54.082355Z",
     "shell.execute_reply": "2023-08-22T13:30:54.081755Z",
     "shell.execute_reply.started": "2023-08-22T13:29:03.580967Z"
    }
   },
   "outputs": [],
   "source": [
    "nmnist_dl = spyx.data.NMNIST_loader(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d127b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:04.100342Z",
     "iopub.status.busy": "2023-08-22T13:31:04.099894Z",
     "iopub.status.idle": "2023-08-22T13:31:10.825725Z",
     "shell.execute_reply": "2023-08-22T13:31:10.825122Z",
     "shell.execute_reply.started": "2023-08-22T13:31:04.100316Z"
    }
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "x, y = nmnist_dl.train_epoch(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f6dbd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T07:32:32.765873Z",
     "iopub.status.busy": "2023-08-22T07:32:32.765583Z",
     "iopub.status.idle": "2023-08-22T07:32:32.769809Z",
     "shell.execute_reply": "2023-08-22T07:32:32.769317Z",
     "shell.execute_reply.started": "2023-08-22T07:32:32.765852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 64, 5, 2, 34, 34)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34e0028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:17.088087Z",
     "iopub.status.busy": "2023-08-22T13:31:17.087472Z",
     "iopub.status.idle": "2023-08-22T13:31:17.092406Z",
     "shell.execute_reply": "2023-08-22T13:31:17.091592Z",
     "shell.execute_reply.started": "2023-08-22T13:31:17.088061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878677e",
   "metadata": {},
   "source": [
    "## SNN\n",
    "\n",
    "Here we define a simple feed-forward SNN using Haiku's RNN features, incorporating our\n",
    "LIF neuron models where activation functions would usually go. Haiku manages all of the state for us, so when we transform the function and get an apply() function we just need to pass the params!\n",
    "\n",
    "Since spiking neurons have a discrete all-or-nothing activation, in order to do gradient descent we'll have to approximate the derivative of the Heaviside function with something smoother. In this case, we use the SuperSpike surrogate gradient from Zenke & Ganguli 2017.\n",
    "Also not that we aren't using bias terms on the linear layers and since the inputs are images, we flatten the data before feeding it to the first layer.\n",
    "\n",
    "Depending on computational constraints, we can use haiku's dynamic unroll to iterate the SNN, or we can use static unroll where the SNN will be unrolled during the JIT compiling process to further increase speed when training on GPU. Note that the static unroll will take longer to compile, but once it runs the iterations per second will be 2x-3x greater than the dynamic unroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f02cfdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:18.772594Z",
     "iopub.status.busy": "2023-08-22T13:31:18.772325Z",
     "iopub.status.idle": "2023-08-22T13:31:18.778256Z",
     "shell.execute_reply": "2023-08-22T13:31:18.777507Z",
     "shell.execute_reply.started": "2023-08-22T13:31:18.772575Z"
    }
   },
   "outputs": [],
   "source": [
    "surrogate = spyx.axn.Axon(spyx.axn.arctan())\n",
    "\n",
    "def nmnist_snn(x):    \n",
    "    # seqs is [T, F].\n",
    "    \n",
    "    x = hk.BatchApply(hk.Flatten())(x)\n",
    "    x = hk.BatchApply(hk.Linear(512, with_bias=False))(x)\n",
    "    \n",
    "    core = hk.DeepRNN([\n",
    "        snn.LIF((512,), activation=surrogate),\n",
    "        hk.Linear(10, with_bias=False),\n",
    "        snn.LI((10,))\n",
    "    ])\n",
    "    \n",
    "    # static unroll for maximum performance\n",
    "    spikes, V = hk.dynamic_unroll(core, x, core.initial_state(x.shape[0]), time_major=False, unroll=20)\n",
    "    \n",
    "    return spikes, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711ce25",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We define a training loop below.\n",
    "\n",
    "We use the Lion optimizer from Optax, which is a more efficient competitor to the popular Adam. The eval steps and updates are JIT'ed to maximize time spent in optimized GPU code and minimize time spent in higher-level python.\n",
    "\n",
    "The use of regularizers in the spiking network will be covered in a seperate tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa93e1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:21.066862Z",
     "iopub.status.busy": "2023-08-22T13:31:21.066410Z",
     "iopub.status.idle": "2023-08-22T13:31:21.075433Z",
     "shell.execute_reply": "2023-08-22T13:31:21.074805Z",
     "shell.execute_reply.started": "2023-08-22T13:31:21.066838Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd(SNN, params, dl, epochs=300, schedule = 2e-4):\n",
    "    \n",
    "    #aug = spyx.data.shift_augment(max_shift=3, axes=(-2,-1)) # need to make this stateless\n",
    "    \n",
    "    opt = optax.chain(\n",
    "        optax.centralize(),\n",
    "        optax.lion(learning_rate=schedule)\n",
    "    )\n",
    "    # create and initialize the optimizer\n",
    "    opt_state = opt.init(params)\n",
    "    grad_params = params\n",
    "        \n",
    "    # define and compile our eval function that computes the loss for our SNN\n",
    "    @jax.jit\n",
    "    def net_eval(weights, events, targets):\n",
    "        readout = SNN.apply(weights, events)\n",
    "        traces, V_f = readout\n",
    "        return spyx.fn.integral_crossentropy(traces, targets, smoothing=0.2)\n",
    "        \n",
    "    # Use JAX to create a function that calculates the loss and the gradient!\n",
    "    surrogate_grad = jax.value_and_grad(net_eval) \n",
    "        \n",
    "    rng = jax.random.PRNGKey(0)        \n",
    "    \n",
    "    # compile the meat of our training loop for speed\n",
    "    @jax.jit\n",
    "    def train_step(state, data):\n",
    "        grad_params, opt_state = state\n",
    "        events, targets = data # fix this\n",
    "        events = jnp.unpackbits(events, axis=1) # decompress temporal axis\n",
    "        # compute loss and gradient                    # need better augment rng\n",
    "        loss, grads = surrogate_grad(grad_params, events, targets)\n",
    "        # generate updates based on the gradients and optimizer\n",
    "        updates, opt_state = opt.update(grads, opt_state, grad_params)\n",
    "        # return the updated parameters\n",
    "        new_state = [optax.apply_updates(grad_params, updates), opt_state]\n",
    "        return new_state, loss\n",
    "    \n",
    "    # For validation epochs, do the same as before but compute the\n",
    "    # accuracy, predictions and losses (no gradients needed)\n",
    "    @jax.jit\n",
    "    def eval_step(grad_params, data):\n",
    "        events, targets = data # fix\n",
    "        events = jnp.unpackbits(events, axis=1)\n",
    "        readout = SNN.apply(grad_params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.fn.integral_accuracy(traces, targets)\n",
    "        loss = spyx.fn.integral_crossentropy(traces, targets)\n",
    "        return grad_params, jnp.array([acc, loss])\n",
    "        \n",
    "    \n",
    "    val_data = dl.val_epoch()\n",
    "    \n",
    "    # Here's the start of our training loop!\n",
    "    @scan_tqdm(epochs)\n",
    "    def epoch(epoch_state, epoch_num):\n",
    "        curr_params, curr_opt_state = epoch_state\n",
    "        \n",
    "        shuffle_rng = jax.random.fold_in(rng, epoch_num)\n",
    "        train_data = dl.train_epoch(shuffle_rng)\n",
    "        \n",
    "        # train epoch\n",
    "        end_state, train_loss = jax.lax.scan(\n",
    "            train_step,# func\n",
    "            [curr_params, curr_opt_state],# init\n",
    "            train_data,# xs\n",
    "            train_data.obs.shape[0]# len\n",
    "        )\n",
    "        \n",
    "        new_params, _ = end_state\n",
    "            \n",
    "        # val epoch\n",
    "        _, val_metrics = jax.lax.scan(\n",
    "            eval_step,# func\n",
    "            new_params,# init\n",
    "            val_data,# xs\n",
    "            val_data.obs.shape[0]# len\n",
    "        )\n",
    "\n",
    "        \n",
    "        return end_state, jnp.concatenate([jnp.expand_dims(jnp.mean(train_loss),0), jnp.mean(val_metrics, axis=0)])\n",
    "    # end epoch\n",
    "    \n",
    "    # epoch loop\n",
    "    final_state, metrics = jax.lax.scan(\n",
    "        epoch,\n",
    "        [grad_params, opt_state], # metric arrays\n",
    "        jnp.arange(epochs), # \n",
    "        epochs # len of loop\n",
    "    )\n",
    "    \n",
    "    final_params, _ = final_state\n",
    "    \n",
    "                \n",
    "    # return our final, optimized network.       \n",
    "    return final_params, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5eb4fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:23.200838Z",
     "iopub.status.busy": "2023-08-22T13:31:23.200223Z",
     "iopub.status.idle": "2023-08-22T13:31:23.205805Z",
     "shell.execute_reply": "2023-08-22T13:31:23.205083Z",
     "shell.execute_reply.started": "2023-08-22T13:31:23.200810Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_gd(SNN, params, dl):\n",
    "\n",
    "    @jax.jit\n",
    "    def test_step(params, data):\n",
    "        events, targets = data\n",
    "        events = jnp.unpackbits(events, axis=1)\n",
    "        readout = SNN.apply(params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.fn.integral_accuracy(traces, targets)\n",
    "        loss = spyx.fn.integral_crossentropy(traces, targets)\n",
    "        return params, [acc, loss, pred, targets]\n",
    "    \n",
    "    test_data = dl.test_epoch()\n",
    "    \n",
    "    _, test_metrics = jax.lax.scan(\n",
    "            test_step,# func\n",
    "            params,# init\n",
    "            test_data,# xs\n",
    "            test_data.obs.shape[0]# len\n",
    "    )\n",
    "    \n",
    "    acc = jnp.mean(test_metrics[0])\n",
    "    loss = jnp.mean(test_metrics[1])\n",
    "    preds = jnp.array(test_metrics[2]).flatten()\n",
    "    tgts = jnp.array(test_metrics[3]).flatten()\n",
    "    return acc, loss, preds, tgts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1669fb3",
   "metadata": {},
   "source": [
    "## Training Time\n",
    "\n",
    "We'll train the network for 50 epochs since SHD is more difficult than MNIST.\n",
    "\n",
    "The SHD dataloader for Spyx has built-in leave-one-group-out cross validation. This is becuase the test set for SHD has two unseen speakers, so when we train our model we need to make it robust to speakers it isn't training on in the hopes of improving generalization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8922b152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:25.204557Z",
     "iopub.status.busy": "2023-08-22T13:31:25.204295Z",
     "iopub.status.idle": "2023-08-22T13:31:25.207500Z",
     "shell.execute_reply": "2023-08-22T13:31:25.206930Z",
     "shell.execute_reply.started": "2023-08-22T13:31:25.204539Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1565db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:31:26.548765Z",
     "iopub.status.busy": "2023-08-22T13:31:26.548504Z",
     "iopub.status.idle": "2023-08-22T13:31:49.296879Z",
     "shell.execute_reply": "2023-08-22T13:31:49.296299Z",
     "shell.execute_reply.started": "2023-08-22T13:31:26.548747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2640db2fbf7c4100baabc02c2e616ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: train_loss=0.901228666305542, val_acc=0.963994562625885, val_loss=1.3109228610992432\n",
      "Time Elapsed: 12.993635416030884\n",
      "Accuracy: 0.9629407 Loss: 1.3118557\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN = hk.without_apply_rng(hk.transform(nmnist_snn))\n",
    "params = SNN.init(rng=key, x=x[0])\n",
    "start = time()\n",
    "grad_params, metrics = gd(SNN, params, nmnist_dl, epochs=25, schedule=2e-4)\n",
    "elapsed = time() - start\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics[-1]))\n",
    "print(\"Time Elapsed:\", elapsed)\n",
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, nmnist_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bed2c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:32:02.948227Z",
     "iopub.status.busy": "2023-08-22T13:32:02.947955Z",
     "iopub.status.idle": "2023-08-22T13:32:15.489672Z",
     "shell.execute_reply": "2023-08-22T13:32:15.489080Z",
     "shell.execute_reply.started": "2023-08-22T13:32:02.948208Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d5182b53624273881ac72e05f42f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: train_loss=0.9087139964103699, val_acc=0.9590693116188049, val_loss=1.331193447113037\n",
      "Time Elapsed: 10.810823678970337\n",
      "Accuracy: 0.9602364 Loss: 1.326659\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(7)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN = hk.without_apply_rng(hk.transform(nmnist_snn))\n",
    "params = SNN.init(rng=key, x=x[0])\n",
    "start = time()\n",
    "grad_params, metrics = gd(SNN, params, nmnist_dl, epochs=25, schedule=2e-4)\n",
    "elapsed = time() - start\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics[-1]))\n",
    "print(\"Time Elapsed:\", elapsed)\n",
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, nmnist_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32cfa958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:32:15.491257Z",
     "iopub.status.busy": "2023-08-22T13:32:15.491051Z",
     "iopub.status.idle": "2023-08-22T13:32:27.480886Z",
     "shell.execute_reply": "2023-08-22T13:32:27.480320Z",
     "shell.execute_reply.started": "2023-08-22T13:32:15.491257Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75cb66a65b4447bb5e20e5498d1cbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: train_loss=0.9158933758735657, val_acc=0.956182062625885, val_loss=1.3441404104232788\n",
      "Time Elapsed: 10.236445426940918\n",
      "Accuracy: 0.96063703 Loss: 1.3356191\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN = hk.without_apply_rng(hk.transform(nmnist_snn))\n",
    "params = SNN.init(rng=key, x=x[0])\n",
    "start = time()\n",
    "grad_params, metrics = gd(SNN, params, nmnist_dl, epochs=25, schedule=2e-4)\n",
    "elapsed = time() - start\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics[-1]))\n",
    "print(\"Time Elapsed:\", elapsed)\n",
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, nmnist_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d48193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:32:27.481719Z",
     "iopub.status.busy": "2023-08-22T13:32:27.481530Z",
     "iopub.status.idle": "2023-08-22T13:32:39.587298Z",
     "shell.execute_reply": "2023-08-22T13:32:39.586652Z",
     "shell.execute_reply.started": "2023-08-22T13:32:27.481702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a9d0fdf7d9438d80f0ca59af9dc2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: train_loss=0.9062016606330872, val_acc=0.962296187877655, val_loss=1.3263999223709106\n",
      "Time Elapsed: 10.234907388687134\n",
      "Accuracy: 0.96063703 Loss: 1.3251297\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(12345)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN = hk.without_apply_rng(hk.transform(nmnist_snn))\n",
    "params = SNN.init(rng=key, x=x[0])\n",
    "start = time()\n",
    "grad_params, metrics = gd(SNN, params, nmnist_dl, epochs=25, schedule=2e-4)\n",
    "elapsed = time() - start\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics[-1]))\n",
    "print(\"Time Elapsed:\", elapsed)\n",
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, nmnist_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f408697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T13:32:39.588825Z",
     "iopub.status.busy": "2023-08-22T13:32:39.588638Z",
     "iopub.status.idle": "2023-08-22T13:32:52.013276Z",
     "shell.execute_reply": "2023-08-22T13:32:52.012555Z",
     "shell.execute_reply.started": "2023-08-22T13:32:39.588810Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b587645453473bab71da438101b25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: train_loss=0.9095025062561035, val_acc=0.9585598111152649, val_loss=1.3340524435043335\n",
      "Time Elapsed: 10.541772603988647\n",
      "Accuracy: 0.95763224 Loss: 1.3337369\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(54321)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN = hk.without_apply_rng(hk.transform(nmnist_snn))\n",
    "params = SNN.init(rng=key, x=x[0])\n",
    "start = time()\n",
    "grad_params, metrics = gd(SNN, params, nmnist_dl, epochs=25, schedule=2e-4) \n",
    "elapsed = time() - start\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics[-1]))\n",
    "print(\"Time Elapsed:\", elapsed)\n",
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, nmnist_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
