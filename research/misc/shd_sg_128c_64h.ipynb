{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3a7575",
   "metadata": {},
   "source": [
    "# Training an SNN using surrogate gradients!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae132f5-74b8-4549-9039-9b3c2e71f922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:48:34.976998Z",
     "iopub.status.busy": "2023-08-22T11:48:34.976353Z",
     "iopub.status.idle": "2023-08-22T11:48:36.315767Z",
     "shell.execute_reply": "2023-08-22T11:48:36.315170Z",
     "shell.execute_reply.started": "2023-08-22T11:48:34.976976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([12.34, 24.68, 37.02], dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "warmup = jnp.array([1,2,3])\n",
    "warmup * 12.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf6fa71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:48:37.363608Z",
     "iopub.status.busy": "2023-08-22T11:48:37.363274Z",
     "iopub.status.idle": "2023-08-22T11:48:39.257738Z",
     "shell.execute_reply": "2023-08-22T11:48:39.257014Z",
     "shell.execute_reply.started": "2023-08-22T11:48:37.363590Z"
    }
   },
   "outputs": [],
   "source": [
    "# implement our SNN in DeepMind's Haiku\n",
    "import haiku as hk\n",
    "\n",
    "# JAX imports\n",
    "import jax\n",
    "import jmp\n",
    "\n",
    "# rendering tools\n",
    "# for surrogate loss training.\n",
    "import optax\n",
    "from jax import numpy as jnp\n",
    "from jax_tqdm import scan_tqdm\n",
    "\n",
    "import spyx\n",
    "import spyx.nn as snn\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9402d",
   "metadata": {},
   "source": [
    "## Set Mixed Precision Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfdcd58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:48:46.407508Z",
     "iopub.status.busy": "2023-08-22T11:48:46.406757Z",
     "iopub.status.idle": "2023-08-22T11:48:46.410941Z",
     "shell.execute_reply": "2023-08-22T11:48:46.410442Z",
     "shell.execute_reply.started": "2023-08-22T11:48:46.407479Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = jmp.get_policy('half')\n",
    "\n",
    "\n",
    "hk.mixed_precision.set_policy(hk.Linear, policy)\n",
    "hk.mixed_precision.set_policy(snn.ALIF, policy)\n",
    "hk.mixed_precision.set_policy(snn.LI, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf2a89",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6573ba59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:48:48.736751Z",
     "iopub.status.busy": "2023-08-22T11:48:48.736024Z",
     "iopub.status.idle": "2023-08-22T11:49:40.542360Z",
     "shell.execute_reply": "2023-08-22T11:49:40.541720Z",
     "shell.execute_reply.started": "2023-08-22T11:48:48.736727Z"
    }
   },
   "outputs": [],
   "source": [
    "shd_dl = spyx.data.SHD_loader(256,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d127b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:50:49.428408Z",
     "iopub.status.busy": "2023-08-22T11:50:49.427741Z",
     "iopub.status.idle": "2023-08-22T11:50:50.664216Z",
     "shell.execute_reply": "2023-08-22T11:50:50.663609Z",
     "shell.execute_reply.started": "2023-08-22T11:50:49.428362Z"
    }
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "x, y = shd_dl.train_epoch(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9bc18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:50:56.721032Z",
     "iopub.status.busy": "2023-08-22T11:50:56.720772Z",
     "iopub.status.idle": "2023-08-22T11:50:56.726639Z",
     "shell.execute_reply": "2023-08-22T11:50:56.726097Z",
     "shell.execute_reply.started": "2023-08-22T11:50:56.721014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878677e",
   "metadata": {},
   "source": [
    "## SNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6946dd93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:51:40.422604Z",
     "iopub.status.busy": "2023-08-22T11:51:40.422032Z",
     "iopub.status.idle": "2023-08-22T11:51:40.427750Z",
     "shell.execute_reply": "2023-08-22T11:51:40.426925Z",
     "shell.execute_reply.started": "2023-08-22T11:51:40.422577Z"
    }
   },
   "outputs": [],
   "source": [
    "surrogate = spyx.axn.Axon(spyx.axn.arctan())\n",
    "\n",
    "def snn_alif(x):\n",
    "    \n",
    "    x = hk.BatchApply(hk.Linear(64, with_bias=False))(x)\n",
    "    \n",
    "    core = hk.DeepRNN([\n",
    "        snn.ALIF((64,), activation=surrogate),\n",
    "        hk.Linear(64, with_bias=False),\n",
    "        snn.ALIF((64,), activation=surrogate),\n",
    "        hk.Linear(20, with_bias=False),\n",
    "        snn.LI((20,))\n",
    "    ])\n",
    "    \n",
    "    # static unroll for maximum performance\n",
    "    spikes, V = hk.dynamic_unroll(core, x, core.initial_state(x.shape[0]), time_major=False, unroll=16)\n",
    "    \n",
    "    return spikes, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711ce25",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We define a training loop below.\n",
    "\n",
    "We use the Lion optimizer from Optax, which is a more efficient competitor to the popular Adam. The eval steps and updates are JIT'ed to maximize time spent in optimized GPU code and minimize time spent in higher-level python.\n",
    "\n",
    "The use of regularizers in the spiking network will be covered in a seperate tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa93e1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:51:00.703292Z",
     "iopub.status.busy": "2023-08-22T11:51:00.702531Z",
     "iopub.status.idle": "2023-08-22T11:51:00.711578Z",
     "shell.execute_reply": "2023-08-22T11:51:00.711098Z",
     "shell.execute_reply.started": "2023-08-22T11:51:00.703266Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd(SNN, params, dl, seed, epochs=300, schedule=4e-4):\n",
    "    \n",
    "    aug = spyx.data.shift_augment(max_shift=8) # need to make this stateless\n",
    "\n",
    "    opt = optax.chain(\n",
    "        optax.centralize(),\n",
    "        optax.lion(learning_rate=schedule),\n",
    "    )\n",
    "    # create and initialize the optimizer\n",
    "    opt_state = opt.init(params)\n",
    "    grad_params = params\n",
    "        \n",
    "    # define and compile our eval function that computes the loss for our SNN\n",
    "    @jax.jit\n",
    "    def net_eval(weights, events, targets):\n",
    "        readout = SNN.apply(weights, events)\n",
    "        traces, V_f = readout\n",
    "        return spyx.fn.integral_crossentropy(traces, targets)\n",
    "        \n",
    "    # Use JAX to create a function that calculates the loss and the gradient!\n",
    "    surrogate_grad = jax.value_and_grad(net_eval) \n",
    "        \n",
    "    rng = seed        \n",
    "    \n",
    "    # compile the meat of our training loop for speed\n",
    "    @jax.jit\n",
    "    def train_step(state, data):\n",
    "        grad_params, opt_state = state\n",
    "        events, targets = data # fix this\n",
    "        events = jnp.unpackbits(events, axis=1) # decompress temporal axis\n",
    "        # compute loss and gradient                    # need better augment rng\n",
    "        loss, grads = surrogate_grad(grad_params, aug(events, jax.random.fold_in(rng,jnp.sum(targets))), targets)\n",
    "        # generate updates based on the gradients and optimizer\n",
    "        updates, opt_state = opt.update(grads, opt_state, grad_params)\n",
    "        # return the updated parameters\n",
    "        new_state = [optax.apply_updates(grad_params, updates), opt_state]\n",
    "        return new_state, loss\n",
    "    \n",
    "    # For validation epochs, do the same as before but compute the\n",
    "    # accuracy, predictions and losses (no gradients needed)\n",
    "    @jax.jit\n",
    "    def eval_step(grad_params, data):\n",
    "        events, targets = data # fix\n",
    "        events = jnp.unpackbits(events, axis=1)\n",
    "        readout = SNN.apply(grad_params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.fn.integral_accuracy(traces, targets)\n",
    "        loss = spyx.fn.integral_crossentropy(traces, targets)\n",
    "        return grad_params, jnp.array([acc, loss])\n",
    "        \n",
    "    \n",
    "    val_data = dl.val_epoch()\n",
    "    \n",
    "    # Here's the start of our training loop!\n",
    "    @scan_tqdm(epochs)\n",
    "    def epoch(epoch_state, epoch_num):\n",
    "        curr_params, curr_opt_state = epoch_state\n",
    "        \n",
    "        shuffle_rng = jax.random.fold_in(rng, epoch_num)\n",
    "        train_data = dl.train_epoch(shuffle_rng)\n",
    "        \n",
    "        # train epoch\n",
    "        end_state, train_loss = jax.lax.scan(\n",
    "            train_step,# func\n",
    "            [curr_params, curr_opt_state],# init\n",
    "            train_data,# xs\n",
    "            train_data.obs.shape[0]# len\n",
    "        )\n",
    "        \n",
    "        new_params, _ = end_state\n",
    "            \n",
    "        # val epoch\n",
    "        _, val_metrics = jax.lax.scan(\n",
    "            eval_step,# func\n",
    "            new_params,# init\n",
    "            val_data,# xs\n",
    "            val_data.obs.shape[0]# len\n",
    "        )\n",
    "\n",
    "        \n",
    "        return end_state, jnp.concatenate([jnp.expand_dims(jnp.mean(train_loss),0), jnp.mean(val_metrics, axis=0)])\n",
    "    # end epoch\n",
    "    \n",
    "    # epoch loop\n",
    "    final_state, metrics = jax.lax.scan(\n",
    "        epoch,\n",
    "        [grad_params, opt_state], # metric arrays\n",
    "        jnp.arange(epochs), # \n",
    "        epochs # len of loop\n",
    "    )\n",
    "    \n",
    "    final_params, _ = final_state\n",
    "    \n",
    "                \n",
    "    # return our final, optimized network.       \n",
    "    return final_params, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5eb4fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:51:06.271856Z",
     "iopub.status.busy": "2023-08-22T11:51:06.271076Z",
     "iopub.status.idle": "2023-08-22T11:51:06.276541Z",
     "shell.execute_reply": "2023-08-22T11:51:06.276041Z",
     "shell.execute_reply.started": "2023-08-22T11:51:06.271830Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_gd(SNN, params, dl):\n",
    "\n",
    "    @jax.jit\n",
    "    def test_step(params, data):\n",
    "        events, targets = data\n",
    "        events = jnp.unpackbits(events, axis=1)\n",
    "        readout = SNN.apply(params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.fn.integral_accuracy(traces, targets)\n",
    "        loss = spyx.fn.integral_crossentropy(traces, targets)\n",
    "        return params, [acc, loss, pred, targets]\n",
    "    \n",
    "    test_data = dl.test_epoch()\n",
    "    \n",
    "    _, test_metrics = jax.lax.scan(\n",
    "            test_step,# func\n",
    "            params,# init\n",
    "            test_data,# xs\n",
    "            test_data.obs.shape[0]# len\n",
    "    )\n",
    "    \n",
    "    acc = jnp.mean(test_metrics[0])\n",
    "    loss = jnp.mean(test_metrics[1])\n",
    "    preds = jnp.array(test_metrics[2]).flatten()\n",
    "    tgts = jnp.array(test_metrics[3]).flatten()\n",
    "    return acc, loss, preds, tgts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1669fb3",
   "metadata": {},
   "source": [
    "## Training Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203e21f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:51:08.319169Z",
     "iopub.status.busy": "2023-08-22T11:51:08.318333Z",
     "iopub.status.idle": "2023-08-22T11:51:08.321580Z",
     "shell.execute_reply": "2023-08-22T11:51:08.321198Z",
     "shell.execute_reply.started": "2023-08-22T11:51:08.319143Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb0f0a",
   "metadata": {},
   "source": [
    "# Seed: 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0472c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:51:42.973742Z",
     "iopub.status.busy": "2023-08-22T11:51:42.972691Z",
     "iopub.status.idle": "2023-08-22T11:53:51.126283Z",
     "shell.execute_reply": "2023-08-22T11:53:51.125638Z",
     "shell.execute_reply.started": "2023-08-22T11:51:42.973707Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8d914328344d8cb0502fd588b93a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.79465556144714\n",
      "Performance: train_loss=2.001251220703125, val_acc=0.7584635615348816, val_loss=2.007822036743164\n",
      "Accuracy: 0.68847656 Loss: 2.1133943\n"
     ]
    }
   ],
   "source": [
    "schedule = 2e-4\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN_alif = hk.without_apply_rng(hk.transform(snn_alif))\n",
    "params_alif = SNN_alif.init(rng=key, x=x[0])\n",
    "\n",
    "start = time()\n",
    "grad_params_alif, metrics_alif = gd(SNN_alif, params_alif, shd_dl, key, epochs=500, schedule=schedule) # 1:09\n",
    "elapsed = time() - start\n",
    "print(elapsed)\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics_alif[-1]))\n",
    "acc, loss, preds, tgts = test_gd(SNN_alif, grad_params_alif, shd_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e6cb1",
   "metadata": {},
   "source": [
    "# Seed: 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf78e7d-f8b9-44ff-9bf2-5ebdb0589051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:53:51.127752Z",
     "iopub.status.busy": "2023-08-22T11:53:51.127549Z",
     "iopub.status.idle": "2023-08-22T11:56:00.655091Z",
     "shell.execute_reply": "2023-08-22T11:56:00.654595Z",
     "shell.execute_reply.started": "2023-08-22T11:53:51.127734Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1af8fdae184c61bfc0d1ca335da838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.25066351890564\n",
      "Performance: train_loss=2.056994676589966, val_acc=0.767578125, val_loss=2.009467601776123\n",
      "Accuracy: 0.6660156 Loss: 2.147407\n"
     ]
    }
   ],
   "source": [
    "schedule = 2e-4\n",
    "\n",
    "key = jax.random.PRNGKey(12345)\n",
    "\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN_alif = hk.without_apply_rng(hk.transform(snn_alif))\n",
    "params_alif = SNN_alif.init(rng=key, x=x[0])\n",
    "\n",
    "start = time()\n",
    "grad_params_alif, metrics_alif = gd(SNN_alif, params_alif, shd_dl, key, epochs=500, schedule=schedule) # 1:09\n",
    "elapsed = time() - start\n",
    "print(elapsed)\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics_alif[-1]))\n",
    "acc, loss, preds, tgts = test_gd(SNN_alif, grad_params_alif, shd_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389c2ef",
   "metadata": {},
   "source": [
    "# Seed: 54321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f17e55f-9f38-475c-b552-4f99f387b54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:56:00.667313Z",
     "iopub.status.busy": "2023-08-22T11:56:00.667089Z",
     "iopub.status.idle": "2023-08-22T11:58:14.111799Z",
     "shell.execute_reply": "2023-08-22T11:58:14.109574Z",
     "shell.execute_reply.started": "2023-08-22T11:56:00.667294Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec9440cf7d24f4983aa586623103f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.2600417137146\n",
      "Performance: train_loss=2.046523094177246, val_acc=0.7584635615348816, val_loss=2.0117335319519043\n",
      "Accuracy: 0.64941406 Loss: 2.167109\n"
     ]
    }
   ],
   "source": [
    "schedule = 2e-4\n",
    "\n",
    "key = jax.random.PRNGKey(54321)\n",
    "\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN_alif = hk.without_apply_rng(hk.transform(snn_alif))\n",
    "params_alif = SNN_alif.init(rng=key, x=x[0])\n",
    "\n",
    "start = time()\n",
    "grad_params_alif, metrics_alif = gd(SNN_alif, params_alif, shd_dl, key, epochs=500, schedule=schedule) # 1:09\n",
    "elapsed = time() - start\n",
    "print(elapsed)\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics_alif[-1]))\n",
    "acc, loss, preds, tgts = test_gd(SNN_alif, grad_params_alif, shd_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae647709",
   "metadata": {},
   "source": [
    "# Seed: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777158d8-37c8-4c1a-9701-815c69f3cbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T11:58:14.116396Z",
     "iopub.status.busy": "2023-08-22T11:58:14.116247Z",
     "iopub.status.idle": "2023-08-22T12:00:26.614837Z",
     "shell.execute_reply": "2023-08-22T12:00:26.614065Z",
     "shell.execute_reply.started": "2023-08-22T11:58:14.116379Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c78e1699c84cfba41d2726b060d591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.34287238121033\n",
      "Performance: train_loss=2.002713203430176, val_acc=0.7747396230697632, val_loss=1.9932903051376343\n",
      "Accuracy: 0.69433594 Loss: 2.099601\n"
     ]
    }
   ],
   "source": [
    "schedule = 2e-4\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN_alif = hk.without_apply_rng(hk.transform(snn_alif))\n",
    "params_alif = SNN_alif.init(rng=key, x=x[0])\n",
    "\n",
    "start = time()\n",
    "grad_params_alif, metrics_alif = gd(SNN_alif, params_alif, shd_dl, key, epochs=500, schedule=schedule) # 1:09\n",
    "elapsed = time() - start\n",
    "print(elapsed)\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics_alif[-1]))\n",
    "acc, loss, preds, tgts = test_gd(SNN_alif, grad_params_alif, shd_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8904036",
   "metadata": {},
   "source": [
    "# Seed: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21715245-16fe-4a98-8f2a-460c141a2e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T12:00:26.618857Z",
     "iopub.status.busy": "2023-08-22T12:00:26.618702Z",
     "iopub.status.idle": "2023-08-22T12:02:40.441986Z",
     "shell.execute_reply": "2023-08-22T12:02:40.441194Z",
     "shell.execute_reply.started": "2023-08-22T12:00:26.618841Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2e94c39cea4a4d8623264751901c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.24864721298218\n",
      "Performance: train_loss=1.9891881942749023, val_acc=0.7825521230697632, val_loss=1.9762778282165527\n",
      "Accuracy: 0.73583984 Loss: 2.0580053\n"
     ]
    }
   ],
   "source": [
    "schedule = 2e-4\n",
    "\n",
    "key = jax.random.PRNGKey(7)\n",
    "\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "SNN_alif = hk.without_apply_rng(hk.transform(snn_alif))\n",
    "params_alif = SNN_alif.init(rng=key, x=x[0])\n",
    "\n",
    "start = time()\n",
    "grad_params_alif, metrics_alif = gd(SNN_alif, params_alif, shd_dl, key, epochs=500, schedule=schedule) # 1:09\n",
    "elapsed = time() - start\n",
    "print(elapsed)\n",
    "print(\"Performance: train_loss={}, val_acc={}, val_loss={}\".format(*metrics_alif[-1]))\n",
    "acc, loss, preds, tgts = test_gd(SNN_alif, grad_params_alif, shd_dl)\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
